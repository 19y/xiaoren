[{"题干":"已知数个点的横纵坐标，要分析y和x是否有线性关系，下面哪个图比较适合？","选项F":"","选项E":"","选项D":"都不是","答案":"A","选项C":"直方图","选项B":"条形图","选项A":"散点图","类型":"1-单选"},{"题干":"逻辑回归算法属于以下什么算法","选项F":"","选项E":"","选项D":"支持向量机算法","答案":"A","选项C":"神经网络算法","选项B":"聚类算法","选项A":"分类算法","类型":"1-单选"},{"题干":"代价函数是（）","选项F":"","选项E":"","选项D":"所有预测样本标签值的平方和","答案":"B","选项C":"训练样本与真实样本的差值","选项B":"所有训练样本估计误差的平方和","选项A":"所有训练样本标签值的平方和","类型":"1-单选"},{"题干":"可以通过（）寻找代价函数最小值","选项F":"","选项E":"","选项D":"线性回归方法","答案":"A","选项C":"直接让代价函数为零","选项B":"对代价函数求导","选项A":"梯度下降算法","类型":"1-单选"},{"题干":"梯度下降算法是一个（）的算法","选项F":"","选项E":"","选项D":"迭代求解代价函数最小值","答案":"D","选项C":"求解函数最大值","选项B":"一次求解","选项A":"迭代优化","类型":"1-单选"},{"题干":"设A=[[-2,1],[1,-1]]则A的逆矩阵是","选项F":"","选项E":"","选项D":"其他都不对","答案":"C","选项C":"[[-1,-1],[-1,-2]]","选项B":"[[2,1],[1,1]]","选项A":"[[1,1],[1,2]]","类型":"1-单选"},{"题干":"监督学习要有（）","选项F":"","选项E":"","选项D":"标签","答案":"A","选项C":"大量结果","选项B":"大量特征","选项A":"训练样本与其相应的真实结果","类型":"1-单选"},{"题干":"大数据:代价函数是（）","选项F":"","选项E":"","选项D":"所有预测样本标签值的平方和","答案":"B","选项C":"训练样本与真实样本的差值","选项B":"所有训练样本估计误差的平方和","选项A":"所有训练样本标签值的平方和","类型":"1-单选"},{"题干":"大数据:通常来说，下面哪种（些）方法能够用来预测连续因变量？\n1.    线性回归\n2.    逻辑回归","选项F":"","选项E":"","选项D":"以上皆非","答案":"B","选项C":"只有2","选项B":"只有1","选项A":"1和2","类型":"1-单选"},{"题干":"大数据:阅读以下代码，求解矩阵a的逆矩阵\nfrom numpy import *\nimport numpy as np\na=mat([[1,2],[2,3]])","选项F":"","选项E":"","选项D":"a.dot","答案":"A","选项C":"a.view","选项B":"a.T","选项A":"a.I","类型":"1-单选"},{"题干":"大数据:设A=[[-2,1],[1,-1]]则A的逆矩阵是","选项F":"","选项E":"","选项D":"其他都不对","答案":"C","选项C":"[[-1,-1],[-1,-2]]","选项B":"[[2,1],[1,1]]","选项A":"[[1,1],[1,2]]","类型":"1-单选"},{"题干":"大数据:已知矩阵A为[[a,b],[c,d],[e,f]],a,b,c,d,e,f均为常量，则A的转置矩阵为","选项F":"","选项E":"","选项D":"[[c,e,a],[b,d,f]]","答案":"A","选项C":"[[c,e,a],[f,b,d]]","选项B":"[[a,e,c],[d,b,f]]","选项A":"[[a,c,e],[b,d,f]]","类型":"1-单选"},{"题干":"大数据:监督学习要有（）","选项F":"","选项E":"","选项D":"标签","答案":"A","选项C":"大量结果","选项B":"大量特征","选项A":"训练样本与其相应的真实结果","类型":"1-单选"},{"题干":"在机器学习中，代价函数是（）","选项F":"","选项E":"","选项D":"所有预测样本标签值的平方和","答案":"B","选项C":"训练样本与真实样本的差值","选项B":"所有训练样本估计误差的平方和","选项A":"所有训练样本标签值的平方和","类型":"1-单选"},{"题干":"在机器学习算法中，可以通过（）寻找代价函数最小值","选项F":"","选项E":"","选项D":"线性回归方法","答案":"A","选项C":"直接让代价函数为零","选项B":"对代价函数求导","选项A":"梯度下降算法","类型":"1-单选"},{"题干":"机器学习中，梯度下降算法是一个（）的算法","选项F":"","选项E":"","选项D":"迭代求解代价函数最小值","答案":"D","选项C":"求解函数最大值","选项B":"一次求解","选项A":"迭代优化","类型":"1-单选"},{"题干":"在机器学习中，通常来说，下面哪种（些）方法能够用来预测连续因变量？\n1.    线性回归\n2.    逻辑回归","选项F":"","选项E":"","选项D":"以上皆非","答案":"B","选项C":"只有2","选项B":"只有1","选项A":"1和2","类型":"1-单选"},{"题干":"在机器学习中，阅读以下代码，求解矩阵a的逆矩阵\nfrom numpy import *\nimport numpy as np\na=mat([[1,2],[2,3]])","选项F":"","选项E":"","选项D":"a.dot","答案":"A","选项C":"a.view","选项B":"a.T","选项A":"a.I","类型":"1-单选"},{"题干":"在机器学习中，设A=[[-2,1],[1,-1]]则A的逆矩阵是","选项F":"","选项E":"","选项D":"其他都不对","答案":"C","选项C":"[[-1,-1],[-1,-2]]","选项B":"[[2,1],[1,1]]","选项A":"[[1,1],[1,2]]","类型":"1-单选"},{"题干":"在机器学习中，已知矩阵A为[[a,b],[c,d],[e,f]],a,b,c,d,e,f均为常量，则A的转置矩阵为","选项F":"","选项E":"","选项D":"[[c,e,a],[b,d,f]]","答案":"A","选项C":"[[c,e,a],[f,b,d]]","选项B":"[[a,e,c],[d,b,f]]","选项A":"[[a,c,e],[b,d,f]]","类型":"1-单选"},{"题干":"在机器学习中，监督学习要有（）","选项F":"","选项E":"","选项D":"标签","答案":"A","选项C":"大量结果","选项B":"大量特征","选项A":"训练样本与其相应的真实结果","类型":"1-单选"},{"题干":"已知只有行数相同，列数也相同的矩阵（即同型矩阵）才能相加。根据条件，则矩阵的加法满足以下哪些运算律","选项F":"","选项E":"","选项D":"负矩阵的加法：A-B = A+(-B)","答案":"ABCD","选项C":"零矩阵满足：A+0=A，其中0是与A同型的零矩阵","选项B":"结合律：(A+B)+C=A+(B+C)","选项A":"交换律：A+B=B+A","类型":"2-多选"},{"题干":"下面是一位母亲给儿子作的成长记录：\nx=[3,4,5,6,7,8,9]\ny=[94.8,104.2,108.7,117.8,124.3,130.8,139.1]\n根据以上样本数据，她建立了身高y(cm)与年龄x（周岁）的线性回归方程为y = 7.19x+73.93，下列结论正确的是：\u00A0","选项F":"","选项E":"","选项D":"儿子年龄增加1周岁，身高约增加7.19cm.","答案":"AD","选项C":"儿子10岁时的身高一定是145.83cm；","选项B":"该回归直线必过点(42，117.1)；","选项A":"y与x具有线性相关关系；","类型":"2-多选"},{"题干":"二元分类将因变量可能属于的两个类分别称为","选项F":"","选项E":"","选项D":"右向量","答案":"AB","选项C":"左向量","选项B":"负向类","选项A":"正向类","类型":"2-多选"},{"题干":"大数据:梯度下降算法的每次迭代受到学习率的影响，下面关于学习率描述正确的是","选项F":"","选项E":"","选项D":"如果学习率α过小，每次迭代必定不会减小代价函数，必定会越过局部最小值导致无法收敛","答案":"AB","选项C":"如果学习率α过大，则达到收敛所需的迭代次数必定会非常高","选项B":"如果学习率α过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛","选项A":"如果学习率α过小，则达到收敛所需的迭代次数可能会非常高","类型":"2-多选"},{"题干":"大数据:逻辑回归构建Cost(hθ(x),y)函数的特点是","选项F":"","选项E":"","选项D":"当y=0 ，但hθ 不为0 时，误差随着hθ 的变大而变大","答案":"ABCD","选项C":"当实际的y=0 且hθ 也为0 时，代价为0","选项B":"当y=1 ，但hθ 不为1 时，误差随着hθ 的变小而变大","选项A":"当实际的y=1 且hθ 也为1 时，误差为0","类型":"2-多选"},{"题干":"大数据:g代表逻辑函数，其公式为g(z)=1/(1+e-z)，以下说法正确的是","选项F":"","选项E":"","选项D":"z=0 时 g(z)=0","答案":"ABC","选项C":"z<0 时 g(z)<0.5","选项B":"z>0 时 g(z)>0.5","选项A":"z=0 时 g(z)=0.5","类型":"2-多选"},{"题干":"大数据:下列四个例子中，属于分类问题的例子有：","选项F":"","选项E":"","选项D":"上网下载一组新闻消息","答案":"ABC","选项C":"区别一个肿瘤是恶性还是良性","选项B":"判断一次金融交易是否是欺诈","选项A":"判断一封电子邮件是否是垃圾邮件","类型":"2-多选"},{"题干":"大数据:二元分类将因变量可能属于的两个类分别称为","选项F":"","选项E":"","选项D":"右向量","答案":"AB","选项C":"左向量","选项B":"负向类","选项A":"正向类","类型":"2-多选"},{"题干":"机器学习中，梯度下降算法的每次迭代受到学习率的影响，下面关于学习率描述正确的是","选项F":"","选项E":"","选项D":"如果学习率α过小，每次迭代必定不会减小代价函数，必定会越过局部最小值导致无法收敛","答案":"AB","选项C":"如果学习率α过大，则达到收敛所需的迭代次数必定会非常高","选项B":"如果学习率α过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛","选项A":"如果学习率α过小，则达到收敛所需的迭代次数可能会非常高","类型":"2-多选"},{"题干":"在机器学习中，逻辑回归构建Cost(hθ(x),y)函数的特点是","选项F":"","选项E":"","选项D":"当y=0 ，但hθ 不为0 时，误差随着hθ 的变大而变大","答案":"ABCD","选项C":"当实际的y=0 且hθ 也为0 时，代价为0","选项B":"当y=1 ，但hθ 不为1 时，误差随着hθ 的变小而变大","选项A":"当实际的y=1 且hθ 也为1 时，误差为0","类型":"2-多选"},{"题干":"在机器学习中，g代表逻辑函数，其公式为g(z)=1/(1+e-z)，以下说法正确的是","选项F":"","选项E":"","选项D":"z=0 时 g(z)=0","答案":"ABC","选项C":"z<0 时 g(z)<0.5","选项B":"z>0 时 g(z)>0.5","选项A":"z=0 时 g(z)=0.5","类型":"2-多选"},{"题干":"在机器学习中，下列四个例子中，属于分类问题的例子有：","选项F":"","选项E":"","选项D":"上网下载一组新闻消息","答案":"ABC","选项C":"区别一个肿瘤是恶性还是良性","选项B":"判断一次金融交易是否是欺诈","选项A":"判断一封电子邮件是否是垃圾邮件","类型":"2-多选"},{"题干":"在机器学习中，二元分类将因变量可能属于的两个类分别称为","选项F":"","选项E":"","选项D":"右向量","答案":"AB","选项C":"左向量","选项B":"负向类","选项A":"正向类","类型":"2-多选"},{"题干":"一个应变量Y与多个自变量X间的线形依存关系，称为多元线形回归","选项F":"","选项E":"","选项D":"","答案":"A","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"如果两个变量是相关的，二者必然具有线性关系","选项F":"","选项E":"","选项D":"","答案":"B","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"线性回归的假设模型为h(x)=X.dot(Theta)，其中X为训练样本的特征向量，Theta为模型的参数向量","选项F":"","选项E":"","选项D":"","答案":"A","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"逻辑回归的假设模型为h(x)=g(X.dot(Theta))，其中g(x)为逻辑函数","选项F":"","选项E":"","选项D":"","答案":"A","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"运用梯度下降算法对逻辑回归模型进行参数更新时，得到的结果与线性回归完全一样，就可能说明逻辑回归模型与线性回归模型完全一样","选项F":"","选项E":"","选项D":"","答案":"B","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"逻辑回归的代价函数和线性回归的代价函数完全一样","选项F":"","选项E":"","选项D":"","答案":"B","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"大数据:所有的二阶矩阵都存在可逆矩阵","选项F":"","选项E":"","选项D":"","答案":"B","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"大数据:一个应变量Y与多个自变量X间的线形依存关系，称为多元线形回归","选项F":"","选项E":"","选项D":"","答案":"A","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"大数据:如果两个变量是相关的，二者必然具有线性关系","选项F":"","选项E":"","选项D":"","答案":"B","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"大数据:线性回归的假设模型为h(x)=X.dot(Theta)，其中X为训练样本的特征向量，Theta为模型的参数向量","选项F":"","选项E":"","选项D":"","答案":"A","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"大数据:逻辑回归的假设模型为h(x)=g(X.dot(Theta))，其中g(x)为逻辑函数","选项F":"","选项E":"","选项D":"","答案":"A","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"大数据:运用梯度下降算法对逻辑回归模型进行参数更新时，得到的结果与线性回归完全一样，就可能说明逻辑回归模型与线性回归模型完全一样","选项F":"","选项E":"","选项D":"","答案":"B","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"大数据:逻辑回归的代价函数和线性回归的代价函数完全一样","选项F":"","选项E":"","选项D":"","答案":"B","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"机器学习中，所有的二阶矩阵都存在可逆矩阵","选项F":"","选项E":"","选项D":"","答案":"B","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"在机器学习中，一个因变量Y与多个自变量X间的线形依存关系，称为多元线形回归","选项F":"","选项E":"","选项D":"","答案":"A","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"在机器学习中，如果两个变量是相关的，二者必然具有线性关系","选项F":"","选项E":"","选项D":"","答案":"B","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"在机器学习中，线性回归的假设模型为h(x)=X.dot(Theta)，其中X为训练样本的特征向量，Theta为模型的参数向量","选项F":"","选项E":"","选项D":"","答案":"A","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"在机器学习中，逻辑回归的假设模型为h(x)=g(X.dot(Theta))，其中g(x)为逻辑函数","选项F":"","选项E":"","选项D":"","答案":"A","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"在机器学习中，运用梯度下降算法对逻辑回归模型进行参数更新时，得到的结果与线性回归完全一样，就可能说明逻辑回归模型与线性回归模型完全一样","选项F":"","选项E":"","选项D":"","答案":"B","选项C":"","选项B":"","选项A":"","类型":"0-判断"},{"题干":"在机器学习中，逻辑回归的代价函数和线性回归的代价函数完全一样","选项F":"","选项E":"","选项D":"","答案":"B","选项C":"","选项B":"","选项A":"","类型":"0-判断"}]